# ###########################################################################
#
# DIESES SKRIPT IST EINE ZUSAMMENFASSUNG DER GESAMTEN PROJEKT-LOGIK
# VERSION 2: Inklusive Speichern der Bilder in Ordner.
#
# Es wurde erstellt, um die Kern-Logik für eine mündliche Prüfung zu lernen.
# Es kombiniert alle relevanten Teile in einer Datei.
#
# ###########################################################################


import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Any
import os
import shutil # Wird für das Erstellen/Löschen von Ordnern benötigt


# Notwendig, um den "Klassiker" (den Entscheidungsbaum) zu laden
# und die finale Auswertung (Accuracy, Report) zu machen.
from sklearn.metrics import accuracy_score, classification_report




# ###########################################################################
# TEIL 1: DAS "GEHIRN" (Aus app/models/tree_params.py)
# ###########################################################################
# Dies sind die 18 Merkmale, die der Baum "kennt".
# Index 0 = area_ratio, Index 3 = elongation (diese werden nicht genutzt)
FEATURE_NAMES = ['area_ratio', 'bbox_ratio', 'solidity', 'elongation', 'perimeter_ratio', 'roughness', 'symmetry', 'lightness_mean', 'lightness_std', 'a_mean', 'a_std', 'b_mean', 'b_std', 'dark_fraction', 'bright_fraction', 'yellow_fraction', 'red_fraction', 'laplacian_std']
CLASSES = ['Bruch', 'Farbfehler', 'Normal', 'Rest']
CHILDREN_LEFT = [1, 2, 3, 4, -1, -1, -1, 8, 9, 10, -1, -1, -1, -1, 15, 16, 17, 18, 19, -1, -1, 22, 23, -1, -1, 26, 27, -1, -1, -1, 31, -1, -1, 34, 35, 36, -1, -1, 39, -1, -1, 42, 43, -1, -1, 46, -1, 48, -1, -1, 51, 52, 53, 54, 55, 56, 57, 58, -1, 60, -1, -1, 63, 64, -1, -1, -1, -1, 69, 70, 71, -1, -1, 74, 75, 76, 77, -1, -1, -1, 81, -1, -1, 84, -1, -1, 87, -1, -1, -1, 91, -1, 93, -1, -1, 96, 97, -1, -1, -1, 101, 102, -1, -1, -1]
CHILDREN_RIGHT = [14, 7, 6, 5, -1, -1, -1, 13, 12, 11, -1, -1, -1, -1, 50, 33, 30, 21, 20, -1, -1, 25, 24, -1, -1, 29, 28, -1, -1, -1, 32, -1, -1, 41, 38, 37, -1, -1, 40, -1, -1, 45, 44, -1, -1, 47, -1, 49, -1, -1, 100, 95, 90, 89, 68, 67, 62, 59, -1, 61, -1, -1, 66, 65, -1, -1, -1, -1, 86, 73, 72, -1, -1, 83, 80, 79, 78, -1, -1, -1, 82, -1, -1, 85, -1, -1, 88, -1, -1, -1, 92, -1, 94, -1, -1, 99, 98, -1, -1, -1, 104, 103, -1, -1, -1]
FEATURE_INDEX = [2, 9, 10, 6, -2, -2, -2, 13, 17, 17, -2, -2, -2, -2, 17, 16, 13, 14, 7, -2, -2, 15, 2, -2, -2, 8, 1, -2, -2, -2, 11, -2, -2, 2, 16, 10, -2, -2, 15, -2, -2, 11, 5, -2, -2, 15, -2, 4, -2, -2, 16, 12, 17, 1, 17, 17, 16, 17, -2, 4, -2, -2, 15, 5, -2, -2, -2, -2, 12, 13, 17, -2, -2, 8, 13, 14, 7, -2, -2, -2, 13, -2, -2, 2, -2, -2, 4, -2, -2, -2, 14, -2, 12, -2, -2, 7, 14, -2, -2, -2, 15, 17, -2, -2, -2]
THRESHOLD = [0.9955520927906036, 131.5915985107422, 13.616559982299805, 0.9175613820552826, -2.0, -2.0, -2.0, 0.24896346032619476, 9.21780252456665, 7.637305974960327, -2.0, -2.0, -2.0, -2.0, 10.86803674697876, 0.001338413916528225, 0.2574382871389389, 0.5802032649517059, 187.76646423339844, -2.0, -2.0, 0.49632875621318817, 0.9970822632312775, -2.0, -2.0, 55.178810119628906, 0.18810483068227768, -2.0, -2.0, -2.0, 149.81436157226562, -2.0, -2.0, 0.9966864287853241, 0.003776826779358089, 13.247979640960693, -2.0, -2.0, 0.5476444363594055, -2.0, -2.0, 149.40938568115234, 0.9475561678409576, -2.0, -2.0, 0.5929132401943207, -2.0, 0.39515142142772675, -2.0, -2.0, 0.009260170627385378, 8.794095993041992, 14.305971145629883, 0.2043825015425682, 11.890393733978271, 11.884346961975098, 0.0015030374634079635, 11.874729633331299, -2.0, 0.39820411801338196, -2.0, -2.0, 0.5288403630256653, 0.9487787485122681, -2.0, -2.0, -2.0, -2.0, 8.749111652374268, 0.22905473411083221, 13.036755561828613, -2.0, -2.0, 62.772315979003906, 0.26109182834625244, 0.590891033411026, 189.97662353515625, -2.0, -2.0, -2.0, 0.26181142032146454, -2.0, -2.0, 0.9971687197685242, -2.0, -2.0, 0.40152543783187866, -2.0, -2.0, -2.0, 0.6149595081806183, -2.0, 8.022324085235596, -2.0, -2.0, 189.08746337890625, 0.5091280341148376, -2.0, -2.0, -2.0, 0.5826407074928284, 13.531453132629395, -2.0, -2.0, -2.0]
VALUE = [[0.05, 0.08333333333333333, 0.8333333333333334, 0.03333333333333333], [0.2727272727272727, 0.12121212121212122, 0.0, 0.6060606060606061], [0.6153846153846154, 0.23076923076923078, 0.0, 0.15384615384615385], [0.8, 0.0, 0.0, 0.2], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.05, 0.05, 0.0, 0.9], [0.05263157894736842, 0.0, 0.0, 0.9473684210526315], [0.5, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [0.037037037037037035, 0.08112874779541446, 0.8818342151675485, 0.0], [0.2, 0.38461538461538464, 0.4153846153846154, 0.0], [0.15789473684210525, 0.18421052631578946, 0.6578947368421053, 0.0], [0.0625, 0.15625, 0.78125, 0.0], [0.5, 0.5, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.10714285714285714, 0.8928571428571429, 0.0], [0.0, 0.6666666666666666, 0.3333333333333333, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.04, 0.96, 0.0], [0.0, 0.25, 0.75, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.6666666666666666, 0.3333333333333333, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.25925925925925924, 0.6666666666666666, 0.07407407407407407, 0.0], [0.6, 0.3, 0.1, 0.0], [0.8571428571428571, 0.14285714285714285, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.6666666666666666, 0.3333333333333333, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.058823529411764705, 0.8823529411764706, 0.058823529411764705, 0.0], [0.5, 0.5, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.9333333333333333, 0.06666666666666667, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.5, 0.5, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.01593625498007968, 0.04183266932270916, 0.9422310756972112, 0.0], [0.0163265306122449, 0.026530612244897958, 0.9571428571428572, 0.0], [0.012658227848101266, 0.016877637130801686, 0.9704641350210971, 0.0], [0.008528784648187633, 0.014925373134328358, 0.976545842217484, 0.0], [0.008547008547008548, 0.01282051282051282, 0.9786324786324786, 0.0], [0.030303030303030304, 0.045454545454545456, 0.9242424242424242, 0.0], [0.015384615384615385, 0.046153846153846156, 0.9384615384615385, 0.0], [0.0, 0.018518518518518517, 0.9814814814814815, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.5, 0.5, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.09090909090909091, 0.18181818181818182, 0.7272727272727273, 0.0], [0.3333333333333333, 0.6666666666666666, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.004975124378109453, 0.007462686567164179, 0.9875621890547264, 0.0], [0.005012531328320802, 0.005012531328320802, 0.9899749373433584, 0.0], [0.0, 0.1, 0.9, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.005141388174807198, 0.002570694087403599, 0.9922879177377892, 0.0], [0.005277044854881266, 0.0, 0.9947229551451188, 0.0], [0.0027548209366391185, 0.0, 0.9972451790633609, 0.0], [0.030303030303030304, 0.0, 0.9696969696969697, 0.0], [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0625, 0.0, 0.9375, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.1, 0.9, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.3333333333333333, 0.6666666666666666, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.4, 0.2, 0.4, 0.0], [0.0, 0.0, 1.0, 0.0], [0.6666666666666666, 0.3333333333333333, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.125, 0.3125, 0.5625, 0.0], [0.2857142857142857, 0.7142857142857143, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.6666666666666666, 0.3333333333333333, 0.0], [0.0, 0.8888888888888888, 0.1111111111111111, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0]]




# ###########################################################################
# TEIL 2: DER KLASSIFIKATOR (Aus app/steps/classifier.py)
# ###########################################################################


@dataclass(frozen=True)
class TreeModel:
    """Datencontainer für das Baum-Modell."""
    feature_names: List[str]
    classes: List[str]
    children_left: List[int]
    children_right: List[int]
    feature_index: List[int]
    threshold: List[float]
    value: List[List[float]]


class RuleBasedClassifier:
    """
    Dies ist der "handprogrammierte" Klassifikator.
    Er braucht KEIN scikit-learn, um Vorhersagen zu treffen.
    Er nutzt nur die Parameter aus TEIL 1.
    """
    def __init__(self) -> None:
        self.model = TreeModel(
            feature_names=list(FEATURE_NAMES),
            classes=list(CLASSES),
            children_left=list(CHILDREN_LEFT),
            children_right=list(CHILDREN_RIGHT),
            feature_index=list(FEATURE_INDEX),
            threshold=list(THRESHOLD),
            value=list(VALUE),
        )


    def predict(self, features: pd.DataFrame) -> np.ndarray:
        """Sagt eine ganze Tabelle (DataFrame) von Merkmalen voraus."""
       
        # Konvertiert die Spalten 'solidity', 'a_mean' etc. in ein numpy-Array
        data = features[self.model.feature_names].to_numpy()
        predictions: List[str] = []
        for row in data:
            # Sagt jede Zeile (jedes Bild) einzeln voraus
            predictions.append(self._predict_row(row))
        return np.array(predictions, dtype=str)


    def _predict_row(self, row: np.ndarray) -> str:
        """
        Hier passiert die "Magie". Folgt dem Baum bis zu einem Blatt.
        Das ist die Kern-Logik für die Prüfung.
        """
        node = 0  # Fange bei der Wurzel (Node 0) an
        while True:
            # 1. Welches Merkmal (Index) müssen wir an dieser Node prüfen?
            feature_idx = self.model.feature_index[node]
           
            # 2. Prüfen: Sind wir an einem "Blatt" (Ende) angekommen?
            if feature_idx == -2:
                # Ja, wir sind am Ende. Finde die Klasse mit den meisten "Votes".
                votes = self.model.value[node]
                class_idx = int(np.argmax(votes))
                return self.model.classes[class_idx] # z.B. "Normal"
           
            # 3. Nein, wir sind an einer "Node" (Entscheidung).
            # Hol den Schwellenwert für diese Entscheidung.
            threshold = self.model.threshold[node]
           
            # 4. Vergleiche den Wert des Bildes mit dem Schwellenwert.
            # z.B. Ist 'symmetry' (Index 6) <= 0.917?
            if row[feature_idx] <= threshold:
                # Wenn ja, gehe nach links im Baum
                node = self.model.children_left[node]
            else:
                # Wenn nein, gehe nach rechts im Baum
                node = self.model.children_right[node]
           
            # (Ein Sicherheitscheck, sollte nie passieren)
            if node == -1:
                raise RuntimeError("Decision tree traversal reached an invalid node.")




# ###########################################################################
# TEIL 3: HINTERGRUND-ENTFERNUNG (Aus app/steps/background_segmentation.py)
# ###########################################################################


@dataclass
class SegmentationResult:
    """Container für die Ergebnisse der Segmentierung."""
    mask: np.ndarray            # Die finale, saubere Maske
    cropped_image: np.ndarray   # Das ausgeschnittene Bild (Vordergrund)
    cropped_mask: np.ndarray    # Die Maske, zugeschnitten auf das Objekt
    bbox: Tuple[int, int, int, int] # Die Bounding Box (x0, y0, x1, y1)
    area_ratio: float           # Flächenanteil (für Prozess-Log)
    raw_mask: np.ndarray        # Die rohe Otsu-Maske (zum Debuggen)
    blurred: np.ndarray         # Das geglättete Bild (zum Debuggen)


class BackgroundSegmenter:
    """
    Entfernt den Hintergrund. Dies ist die "klassische Bildverarbeitung".
    Hier werden KEINE fertigen Masken geladen.
    """
    def __init__(self) -> None:
        # Feste Parameter, wie sie im Projekt-Standard definiert sind
        self.blur_size = 5
        self.median_kernel_size = 5
        self.morph_iterations = 1
        self.close_then_open = True
        self.keep_largest_object = True
        self.invert_threshold = 200
        # Ein 11x11 "Kreis" für die Morphologie
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))


    def segment(self, image: np.ndarray) -> SegmentationResult:
        """Generiert eine Maske und schneidet den Vordergrund aus."""


        # 1. Glätten, um Rauschen zu reduzieren
        blurred = cv2.GaussianBlur(image, (self.blur_size, self.blur_size), 0)
       
        # 2. In LAB-Farbraum umwandeln (L=Helligkeit, a=Rot/Grün, b=Gelb/Blau)
        lab = cv2.cvtColor(blurred, cv2.COLOR_BGR2LAB)
       
        # 3. Nur den Helligkeits-Kanal (L) nehmen
        lightness = lab[:, :, 0]
       
        # 4. Otsu-Schwellwert: Findet automatisch den besten Trennwert
        #    zwischen hellem Objekt und dunklem Hintergrund.
        _, mask = cv2.threshold(lightness, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
       
        # 5. Prüfung: Wenn der Hintergrund heller ist als das Objekt,
        #    (d.h. die Maske ist >200 = fast ganz weiß), drehe sie um.
        if mask.mean() > self.invert_threshold:
            mask = 255 - mask # Invertieren
           
        raw_mask = mask.copy() # Kopie der rohen Maske speichern
       
        # 6. Median-Filter: Entfernt "Salz-und-Pfeffer"-Rauschen
        mask = cv2.medianBlur(mask, self.median_kernel_size)
       
        # 7. Morphologische Operationen: "Säubern" der Maske
        #    CLOSE: Schließt kleine Löcher IM Objekt
        #    OPEN: Entfernt kleine Punkte AUSSERHALB des Objekts
        operations = (cv2.MORPH_CLOSE, cv2.MORPH_OPEN)
       
        for op in operations:
            mask = cv2.morphologyEx(mask, op, self.kernel, iterations=self.morph_iterations)
           
        # 8. Nur das größte Objekt behalten
        cleaned = self._keep_largest_object(mask)
       
        h, w = cleaned.shape
        area_ratio = float(cleaned.sum() / 255) / (h * w)
       
        # 9. Bounding Box finden und zuschneiden
        y0, y1, x0, x1 = self._bounding_box(cleaned)
        if x1 - x0 <= 1 or y1 - y0 <= 1: # Fallback, falls Maske leer
            x0, y0, x1, y1 = 0, 0, w, h
           
        cropped_mask = cleaned[y0:y1, x0:x1]
        cropped_image = image[y0:y1, x0:x1].copy()
       
        # 10. Vordergrund "ausstanzen"
        foreground = cv2.bitwise_and(cropped_image, cropped_image, mask=cropped_mask)
       
        return SegmentationResult(
            mask=cleaned,
            cropped_image=foreground,
            cropped_mask=cropped_mask,
            bbox=(x0, y0, x1, y1),
            area_ratio=area_ratio,
            raw_mask=raw_mask,
            blurred=blurred,
        )


    @staticmethod
    def _keep_largest_object(mask: np.ndarray) -> np.ndarray:
        """Findet alle Konturen und behält nur die größte."""
        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not cnts:
            return np.zeros_like(mask)
        largest = max(cnts, key=cv2.contourArea)
        cleaned = np.zeros_like(mask)
        cv2.drawContours(cleaned, [largest], -1, 255, -1) # Fülle die größte Kontur
        return cleaned


    @staticmethod
    def _bounding_box(mask: np.ndarray) -> Tuple[int, int, int, int]:
        """Findet die min/max Koordinaten der Maske."""
        coords = np.column_stack(np.nonzero(mask))
        if coords.size == 0:
            return (0, 0, mask.shape[1], mask.shape[0])
        ys, xs = coords[:, 0], coords[:, 1]
        return (xs.min(), ys.min(), xs.max() + 1, ys.max() + 1)




# ###########################################################################
# TEIL 4: MERKMALS-EXTRAKTION (Aus app/steps/feature_extraction.py)
# ###########################################################################


def _symmetry_score(mask: np.ndarray) -> float:
    """
    Berechnet den Symmetrie-Score (0-1).
    Kern-Logik für die Symmetrie-Anforderung.
    """
    h, w = mask.shape
    if w == 0 or h == 0: return 0.0
   
    mid = w // 2
    left = mask[:, :mid]
    right = mask[:, mid:]
   
    # Spiegle die rechte Hälfte
    right = np.fliplr(right)
   
    # Stelle sicher, dass beide Hälften gleich breit sind
    min_w = min(left.shape[1], right.shape[1])
    if min_w == 0: return 0.0
   
    # Berechne die absolute Differenz (Pixel-für-Pixel)
    diff = np.abs(left[:, :min_w] - right[:, :min_w]).sum() / 255.0
   
    # Normiere die Differenz auf die Gesamtfläche des Objekts
    content = mask.sum() / 255.0
    if content == 0: return 0.0
   
    score = 1.0 - (diff / content) # 1.0 = perfekt symmetrisch
    return max(0.0, min(1.0, score))


def extract_features(image: np.ndarray, segmentation: SegmentationResult) -> Dict[str, float]:
    """
    Berechnet alle 18 Merkmale für EIN Bild.
    Nutzt die Maske aus TEIL 3.
    """
    mask = segmentation.mask
    if mask.sum() == 0:
        raise ValueError("Segmentierungsmaske ist leer.")


    # --- 1. Form-Merkmale ---
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(cnt)
    perimeter = cv2.arcLength(cnt, True)
    hull = cv2.convexHull(cnt)
    hull_area = cv2.contourArea(hull)
    hull_perimeter = cv2.arcLength(hull, True)
    h, w = mask.shape
    x0, y0, x1, y1 = segmentation.bbox
    bbox_area = max(1, (x1 - x0) * (y1 - y0))
   
    # --- 2. Symmetrie-Merkmal ---
    cropped_mask = segmentation.cropped_mask
    symmetry = _symmetry_score(cropped_mask) # <-- Aufruf der Hilfsfunktion
   
    # --- 3. Farb-Merkmale ---
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    masked_pixels = lab[mask == 255] # Nimm nur Pixel INNERHALB der Maske
    L = masked_pixels[:, 0]
    a = masked_pixels[:, 1]
    b = masked_pixels[:, 2]
   
    # --- 4. Textur-Merkmal ---
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Laplace-Filter findet Kanten.
    laplacian = cv2.Laplacian(gray, cv2.CV_64F, ksize=1)
    # Die Standardabweichung sagt, WIE STARK die Kanten sind.
    laplacian_std = float(np.std(laplacian[mask == 255]))


    # --- 5. Alle 18 Merkmale zusammenbauen ---
    features = {
        # Form
        "area_ratio": float(area / (h * w)), # (ungenutzt)
        "bbox_ratio": float(area / bbox_area),
        "solidity": float(area / hull_area) if hull_area else 0.0,
        "elongation": float((x1 - x0) / ((y1 - y0) + 1e-6)), # (ungenutzt)
        "perimeter_ratio": float(perimeter / (2 * (h + w))),
        "roughness": float(hull_perimeter / (perimeter + 1e-6)),
        # Symmetrie
        "symmetry": float(symmetry),
        # Farbe (Statistik)
        "lightness_mean": float(L.mean()),
        "lightness_std": float(L.std()),
        "a_mean": float(a.mean()),
        "a_std": float(a.std()),
        "b_mean": float(b.mean()),
        "b_std": float(b.std()),
        # Farbe (Anteile, basierend auf Standard-Schwellenwerten)
        "dark_fraction": float((L < 170).mean()),
        "bright_fraction": float((L > 210).mean()),
        "yellow_fraction": float((b > 150).mean()),
        "red_fraction": float((a > 150).mean()),
        # Textur
        "laplacian_std": laplacian_std,
    }
    return features




# ###########################################################################
# TEIL 5: KONFIGURATION & DATENLADEN
# (Kombiniert aus app/config.py und app/steps/dataset_loader.py)
# ###########################################################################


# --- Konfiguration ---
# Hartcodierte Pfade (angenommen 'data' liegt im selben Ordner wie das Skript)
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
NORMAL_DIR = DATA_DIR / "Images" / "Normal"
ANOMALY_DIR = DATA_DIR / "Images" / "Anomaly"
ANNOTATION_FILE = DATA_DIR / "image_anno.csv"


# Die Zielklassen
TARGET_CLASSES = ("Normal", "Farbfehler", "Bruch", "Rest")


# Das "Wörterbuch", um die CSV-Labels auf unsere 4 Klassen zu mappen.
AGGREGATED_CLASS_MAPPING = {
    "normal": "Normal",
    "burnt": "Farbfehler",
    "different colour spot": "Farbfehler",
    "different colour spot,similar colour spot": "Farbfehler",
    "similar colour spot": "Farbfehler",
    "similar colour spot,other": "Farbfehler",
    "similar colour spot,small scratches": "Farbfehler",
    "small scratches": "Farbfehler",
    "corner or edge breakage": "Bruch",
    "corner or edge breakage,small scratches": "Bruch",
    "middle breakage": "Bruch",
    "middle breakage,small scratches": "Bruch",
    "middle breakage,similar colour spot": "Bruch",
    "fryum stuck together": "Rest",
}


# --- NEUE KONFIGURATION FÜR AUSGABEORDNER ---
OUTPUT_ROOT = BASE_DIR / "output"
RUN_IMAGE_DIR = OUTPUT_ROOT / "classified"
MISCLASSIFIED_DIR = RUN_IMAGE_DIR / "Falsch"
TARGET_CLASS_DIRS = {cls: RUN_IMAGE_DIR / cls for cls in TARGET_CLASSES}




# --- Daten-Laden ---
@dataclass(frozen=True)
class ImageRecord:
    """Datencontainer für ein Bild aus der CSV."""
    relative_path: Path
    image_path: Path
    original_label: str
    target_label: str # Die gemappte Klasse (z.B. "Bruch")


def load_dataset() -> List[ImageRecord]:
    """Lädt die Annotation CSV und löst die Dateipfade auf."""


    if not ANNOTATION_FILE.exists():
        raise FileNotFoundError(f"FEHLER: 'image_anno.csv' nicht gefunden unter: {ANNOTATION_FILE}\n"
                                f"Bitte stelle sicher, dass der 'data' Ordner (mit Images/Normal, etc.)\n"
                                f"im selben Verzeichnis wie dieses Skript liegt.")


    annotations = pd.read_csv(ANNOTATION_FILE)
    records: List[ImageRecord] = []
   
    for _, row in annotations.iterrows():
        rel_path = Path(row["image"])
        local_name = rel_path.name
       
        # Finde den korrekten Ordner (Normal oder Anomaly)
        sub_dir = NORMAL_DIR if "Normal" in row["image"] else ANOMALY_DIR
        image_path = sub_dir / local_name
       
        original_label = str(row["label"]).strip()
       
        # Mappe das Label (z.B. "burnt") auf eine Zielklasse (z.B. "Farbfehler")
        try:
            target_label = AGGREGATED_CLASS_MAPPING[original_label]
        except KeyError:
            # Ignoriere Labels, die wir nicht kennen (z.B. "nan")
            continue
           
        records.append(
            ImageRecord(
                relative_path=rel_path,
                image_path=image_path,
                original_label=original_label,
                target_label=target_label,
            )
        )
    return records


# ###########################################################################
# NEU - TEIL 5.5: DATEI-SPEICHERUNG (aus app/steps/result_writer.py)
# ###########################################################################


def _prefixed_name(filename: str, prediction: str, symmetry: float) -> str:
    """
    Erzeugt den Dateinamen mit Symmetrie-Präfix.
    Dies ist eine Anforderung aus der Aufgabenstellung.
    """
    path = Path(filename)
    # Präfix NUR für "Normal" klassifizierte Bilder
    if prediction != "Normal":
        return path.name
       
    # Skaliere Symmetrie-Score (0.0 - 1.0) auf einen Wert von 0-999
    scaled = max(0, min(999, int(round(symmetry * 1000))))
    # Erzeuge Präfix wie "SYM095" (für 9.5% Symmetrie) oder "SYM850"
    prefix = f"SYM{scaled:03d}"
   
    return f"{prefix}_{path.name}"


def save_result(filename: str, prediction: str, target: str, cropped_image: np.ndarray, symmetry: float):
    """
    Speichert das ausgeschnittene Bild in den korrekten Ordner
    (inkl. Symmetrie-Präfix und "Falsch"-Handling).
    """
   
    # 1. Ziel-Ordner bestimmen (z.B. "output/classified/Normal")
    predicted_dir = TARGET_CLASS_DIRS[prediction]
   
    # 2. Dateinamen mit Präfix erstellen (z.B. "SYM850_001.JPG")
    prefixed_name = _prefixed_name(filename, prediction, symmetry)
    out_path = predicted_dir / prefixed_name
   
    # 3. Bild speichern
    cv2.imwrite(str(out_path), cropped_image)
   
    # 4. "Falsch"-Ordner Handling (Anforderung aus Aufgabenstellung)
    if prediction != target:
        # Erzeuge speziellen Dateinamen, z.B.:
        # "001__gt-Normal_pred-Bruch.JPG"
        stem = Path(filename).stem
        suffix = Path(filename).suffix
        mismatch_name = f"{stem}__gt-{target}_pred-{prediction}{suffix}"
        mismatch_path = MISCLASSIFIED_DIR / mismatch_name
       
        # Speichere eine Kopie im "Falsch"-Ordner
        cv2.imwrite(str(mismatch_path), cropped_image)


def _setup_output_dirs():
    """
    Erstellt alle notwendigen Ausgabeordner.
    Löscht alte "classified" Ordner, falls vorhanden.
    """
    if RUN_IMAGE_DIR.exists():
        print(f"Lösche alten Ausgabeordner: {RUN_IMAGE_DIR}")
        shutil.rmtree(RUN_IMAGE_DIR)
       
    print(f"Erstelle Ausgabeordner unter: {RUN_IMAGE_DIR}")
    # Erstellt "output/classified"
    RUN_IMAGE_DIR.mkdir(parents=True, exist_ok=True)
   
    # Erstellt "output/classified/Falsch"
    MISCLASSIFIED_DIR.mkdir(parents=True, exist_ok=True)
   
    # Erstellt "output/classified/Normal", ".../Bruch", etc.
    for d in TARGET_CLASS_DIRS.values():
        d.mkdir(parents=True, exist_ok=True)




# ###########################################################################
# TEIL 6: DER HAUPTABLAUF (MAIN)
# (Angepasst, um Speicherung einzubinden)
# ###########################################################################


def run_classification_pipeline():
    """Führt alle 6 Logik-Schritte nacheinander aus."""
   
    print("Starte Klassifikations-Pipeline (mit Speichern)...")


    # --- SCHRITT 0: Ausgabeordner vorbereiten (aus TEIL 5.5) ---
    _setup_output_dirs()


    # --- SCHRITT 1: Datensatz laden (aus TEIL 5) ---
    try:
        records = load_dataset()
        print(f"Schritt 1: Datensatz mit {len(records)} Bildern geladen.")
    except FileNotFoundError as e:
        print(e)
        return


    # --- SCHRITT 2: Komponenten initialisieren (aus TEIL 2 & 3) ---
    segmenter = BackgroundSegmenter()
    classifier = RuleBasedClassifier()
    print("Schritt 2: Segmentierer und Klassifikator initialisiert.")


    all_features = []
    record_map = {}
    # NEU: Wir brauchen einen Cache für die ausgeschnittenen Bilder
    segmentation_cache: Dict[str, np.ndarray] = {}


    # --- SCHRITT 3: Bilder verarbeiten (Segmentieren & Merkmale extrahieren) ---
    print(f"Schritt 3: Verarbeite {len(records)} Bilder...")
   
    for i, record in enumerate(records):
       
        image = cv2.imread(str(record.image_path))
        if image is None:
            print(f"  Warnung: Bild {record.image_path.name} nicht gefunden. Übersprungen.")
            continue
           
        # Segmentieren (aus TEIL 3)
        segmentation = segmenter.segment(image)
       
        # Merkmale extrahieren (aus TEIL 4)
        try:
            features = extract_features(image, segmentation)
           
            record_id = record.relative_path.as_posix()
            features["record_id"] = record_id
            all_features.append(features)
            record_map[record_id] = record
           
            # NEU: Merke dir das ausgeschnittene Bild für später
            segmentation_cache[record_id] = segmentation.cropped_image
           
        except ValueError as e:
            print(f"  Fehler bei Segmentierung von {record.image_path.name}: {e}. Übersprungen.")
       
        if (i+1) % 50 == 0:
            print(f"  ... {i+1} / {len(records)} verarbeitet.")


    print("Schritt 3: Merkmals-Extraktion abgeschlossen.")


    # --- SCHRITT 4: Klassifizieren (aus TEIL 2) ---
    if not all_features:
        print("Keine Bilder erfolgreich verarbeitet. Pipeline wird beendet.")
        return


    feature_df = pd.DataFrame(all_features).set_index("record_id")


    predictions_array = classifier.predict(feature_df)
   
    feature_df["prediction"] = predictions_array
    print("Schritt 4: Alle Bilder klassifiziert.")


    # --- SCHRITT 5 & 6: Abgleichen, SPEICHERN & Auswerten ---
    print("\n--- AUSWERTUNG & SPEICHERN ---")
   
    ground_truth = []
    predictions = []
   
    for record_id, row in feature_df.iterrows():
        record = record_map[record_id]
       
        true_label = record.target_label
        predicted_label = row["prediction"]
       
        ground_truth.append(true_label)
        predictions.append(predicted_label)
       
        # --- NEUER SPEICHER-AUFRUF (aus TEIL 5.5) ---
        # Hole die benötigten Infos
        symmetry_score = row["symmetry"]
        cropped_image = segmentation_cache[record_id]
        filename = record.image_path.name
       
        # Speichere das Bild
        save_result(filename, predicted_label, true_label, cropped_image, symmetry_score)
       
        # (Fehler-Logging bleibt gleich)
        if true_label != predicted_label:
            print(f"  [FEHLER] {filename} -> Erkannt: '{predicted_label}', Erwartet: '{true_label}'")


    print("Schritt 5 & 6: Speichern und Auswertung abgeschlossen.")


    # --- FINALES ERGEBNIS ---
    print("\n-------------------------------------------------")
    accuracy = accuracy_score(ground_truth, predictions)
    print(f"Genauigkeit: {accuracy * 100:.2f}%")
    print("-------------------------------------------------")
    print("Classification Report (Präzision, Recall, F1):")
    # Zeigt den Report für alle 4 Klassen an
    print(classification_report(ground_truth, predictions, labels=list(TARGET_CLASSES)))
    print("-------------------------------------------------")
    print(f"Alle Bilder wurden in den Ordner '{RUN_IMAGE_DIR}' sortiert.")




# Dieser Code wird nur ausgeführt, wenn du das Skript direkt startest
if __name__ == "__main__":
    run_classification_pipeline()